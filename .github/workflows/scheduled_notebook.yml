name: Daily Scraper
on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:
jobs:
  notebooks:
    name: Run the notebook
    runs-on: ubuntu-latest

    defaults:
      run:
        shell: bash -l {0}

    steps:
      - name: Checkout
        uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10.9'
          # cache: 'pip'

      - name: setup conda
      - uses: conda-incubator/setup-miniconda@v2
        with:
          activate-environment: anaconda-client-env
          environment-file: environment.yml
          auto-activate-base: false
      - run: |
          conda info
          conda list

      # - name: Install Dependencies
      #   run: pip install -r requirements.txt

      - name: Run Script and Update Plot
        run: |
          jupyter nbconvert --to script scheduled_notebook.ipynb
          python scheduled_notebook.ipynb

      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        if: always()
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./exports 
          destination_dir: ./results    

      # - name: Commit and Push Changes
      #   run: |
      #     git config --local user.email "actions@github.com"
      #     git config --local user.name "GitHub Actions"
      #     git add exports/pdokWebsiteDatasets.csv
      #     git commit -m "Finished Scheduled Job: Notebook"
      #     git push origin master